# Ablation Configuration: Numerical + Categorical + Text
# Adds text encoding with XLM-RoBERTa to the baseline

# Model architecture configuration
model:
  # Numerical encoder: 8-dim input → 64-dim output
  # Features: followers_count, following_count, tweet_count, listed_count,
  #           account_age_days, followers_following_ratio, username_length, description_length
  num_input_dim: 8
  num_hidden_dim: 32
  num_output_dim: 64
  
  # Categorical encoder: 5 features → 32-dim output
  # Features: verified, protected, default_avatar, has_url, has_location
  cat_num_categories: [2, 2, 2, 2, 2]
  cat_embedding_dim: 16
  cat_output_dim: 32
  
  # Text encoder (XLM-RoBERTa): text → 256-dim output
  text_model_name: xlm-roberta-base
  text_output_dim: 256
  text_max_length: 128  # 减小以加速训练
  text_freeze_backbone: true  # Freeze backbone during training
  use_precomputed_text_embeddings: true  # 使用预计算嵌入加速训练
  
  # Fusion module: (64 + 32 + 256) → 256-dim output
  fusion_output_dim: 256
  fusion_dropout: 0.1
  fusion_use_attention: true
  
  # ABLATION: Numerical + Categorical + Text
  enabled_modalities: ['num', 'cat', 'text']
  
  # Distance metric for prototype comparison
  distance_metric: euclidean

# Training configuration for meta-learning
training:
  # Episode configuration (N-way K-shot)
  n_way: 2
  k_shot: 10
  n_query: 15
  
  # Episodes per epoch (增加验证episode数量以减少方差)
  n_episodes_train: 100
  n_episodes_val: 100
  
  # Training parameters (降低学习率以稳定训练)
  n_epochs: 200
  learning_rate: 0.0005
  weight_decay: 0.0001
  
  # Text encoder learning rate (smaller than main learning rate)
  text_learning_rate: 0.00001
  
  # Early stopping (增加耐心值)
  patience: 15

# Data paths
data_dir: processed_data
output_dir: results/ablation_num_cat_text

# Reproducibility (null = random seed each run)
seed: null
