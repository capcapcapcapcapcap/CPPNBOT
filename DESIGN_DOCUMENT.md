# åŸºäºåŸå‹ç½‘ç»œçš„è·¨å¹³å°ç¤¾äº¤æœºå™¨äººæ£€æµ‹ç³»ç»Ÿè®¾è®¡æ–‡æ¡£

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

### ç ”ç©¶ç›®æ ‡
è®¾è®¡ä¸€ä¸ªåŸºäºå…ƒå­¦ä¹ çš„è·¨å¹³å°ç¤¾äº¤æœºå™¨äººæ£€æµ‹ç³»ç»Ÿï¼Œèƒ½å¤Ÿåœ¨æ–°å¹³å°ä¸Šé€šè¿‡å°‘é‡æ ‡æ³¨æ ·æœ¬ï¼ˆ5-10ä¸ªï¼‰å¿«é€Ÿéƒ¨ç½²æœºå™¨äººæ£€æµ‹èƒ½åŠ›ï¼Œå®ç°çœŸæ­£çš„"å¿«é€Ÿé€‚åº”"ã€‚

### æ ¸å¿ƒåˆ›æ–°ç‚¹
1. **å¤šæ¨¡æ€åŸå‹å­¦ä¹ **ï¼šç»“åˆæ–‡æœ¬ã€æ•°å€¼ã€å›¾ç»“æ„çš„åŸå‹ç½‘ç»œ
2. **è·¨åŸŸè‡ªåŠ¨é€‚åº”**ï¼šæ¨¡å‹è‡ªåŠ¨å¤„ç†è·¨è¯­è¨€ã€è·¨å¹³å°å·®å¼‚
3. **å°‘æ ·æœ¬æ£€æµ‹**ï¼šæœ€å°åŒ–äººå·¥å¹²é¢„çš„å¿«é€Ÿéƒ¨ç½²èƒ½åŠ›
4. **å®ç”¨æ€§å¯¼å‘**ï¼šé¢å‘çœŸå®åº”ç”¨åœºæ™¯çš„ç³»ç»Ÿè®¾è®¡

### ç ”ç©¶å‡è®¾
- **å‡è®¾1**ï¼šä¸åŒå¹³å°çš„æœºå™¨äººå…·æœ‰å…±åŒçš„è¡Œä¸ºæ¨¡å¼
- **å‡è®¾2**ï¼šè¿™äº›æ¨¡å¼å¯ä»¥é€šè¿‡åŸå‹å­¦ä¹ è¿›è¡Œè·¨åŸŸè¿ç§»
- **å‡è®¾3**ï¼šå°‘é‡æ ·æœ¬è¶³ä»¥æ•æ‰æ–°å¹³å°çš„ç‰¹å¼‚æ€§

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„è®¾è®¡

### æ•´ä½“æ¶æ„å›¾
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        è®­ç»ƒé˜¶æ®µ (Meta-Training)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  æºåŸŸæ•°æ® (Twibot-20)                                           â”‚
â”‚       â†“                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ æ•°æ®è¯»å–å™¨   â”‚â”€â”€â”€â–¶â”‚ å¤šæ¨¡æ€ç¼–ç å™¨  â”‚â”€â”€â”€â–¶â”‚ åŸå‹å­¦ä¹ ç½‘ç»œ     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚       â†“                     â†“                     â†“            â”‚
â”‚  Episodeé‡‡æ ·        ç‰¹å¾æå–ä¸èåˆ        åŸå‹è®¡ç®—ä¸åˆ†ç±»        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        é€‚åº”é˜¶æ®µ (Few-Shot Adaptation)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ç›®æ ‡åŸŸæ•°æ® (Misbot + å°‘é‡æ ‡æ³¨)                                  â”‚
â”‚       â†“                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ æ•°æ®è¯»å–å™¨   â”‚â”€â”€â”€â–¶â”‚ é¢„è®­ç»ƒç¼–ç å™¨  â”‚â”€â”€â”€â–¶â”‚ åŸå‹æ›´æ–°ä¸åˆ†ç±»   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚       â†“                     â†“                     â†“            â”‚
â”‚  æœ€å°é¢„å¤„ç†          ç‰¹å¾è‡ªåŠ¨å¯¹é½        å¿«é€Ÿé€‚åº”æ£€æµ‹            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š æ•°æ®å¤„ç†ç­–ç•¥

### è®¾è®¡åŸåˆ™
- **æœ€å°åŒ–é¢„å¤„ç†**ï¼šåªåšå¿…è¦çš„æ•°æ®è¯»å–ï¼Œé¿å…å¤æ‚çš„ç‰¹å¾å¯¹é½
- **æ¨¡å‹è‡ªé€‚åº”**ï¼šè®©æ¨¡å‹è‡ªåŠ¨å¤„ç†è·¨åŸŸå·®å¼‚
- **çµæ´»æ€§ä¼˜å…ˆ**ï¼šæ”¯æŒä¸åŒå¹³å°çš„æ•°æ®æ ¼å¼å·®å¼‚
- **å€Ÿé‰´ç»éªŒ**ï¼šèåˆTwibotä½œè€…çš„æˆåŠŸç»éªŒä¸è·¨å¹³å°è®¾è®¡ç†å¿µ

### æ ¸å¿ƒåˆ›æ–°ï¼šèåˆç­–ç•¥

æˆ‘ä»¬çš„æ•°æ®å¤„ç†ç­–ç•¥èåˆäº†**Twibotä½œè€…çš„æˆåŠŸç»éªŒ**ä¸**è·¨å¹³å°è®¾è®¡ç†å¿µ**ï¼š

#### **å€Ÿé‰´Twibotä½œè€…çš„ä¼˜ç§€å®è·µ**
```python
# 1. ç‰¹å¾æå–ç­–ç•¥ï¼ˆå€Ÿé‰´ï¼‰
twibot_successful_features = {
    "æ•°å€¼ç‰¹å¾": ["followers_count", "following_count", "listed_count", 
                "username_length", "account_age_days"],  # 5ç»´ç»Ÿä¸€
    "åˆ†ç±»ç‰¹å¾": ["is_verified", "is_protected", "has_default_avatar"],  # 3ç»´ç»Ÿä¸€
    "æ–‡æœ¬å¤„ç†": "description + aggregated_tweets (max 20æ¡)",
    "æ ‡å‡†åŒ–": "Z-score normalization",
    "å›¾å¤„ç†": "åˆ†ç¦»postå…³ç³»ï¼Œä¿ç•™socialå…³ç³»"
}

# 2. æ•°æ®ç»„ç»‡æ–¹å¼ï¼ˆå€Ÿé‰´ï¼‰
twibot_data_organization = {
    "ç´¢å¼•æ˜ å°„": "user_id -> index mapping",
    "åˆ†ç¦»å­˜å‚¨": "ä¸åŒç‰¹å¾ç±»å‹åˆ†åˆ«ä¿å­˜",
    "æ‰¹å¤„ç†": "é«˜æ•ˆçš„æ‰¹é‡æ–‡æœ¬ç¼–ç ",
    "ç¼“å­˜æœºåˆ¶": "é¿å…é‡å¤è®¡ç®—"
}
```

#### **ç»“åˆè·¨å¹³å°è®¾è®¡ç†å¿µ**
```python
# 3. è·¨å¹³å°é€‚é…ï¼ˆåˆ›æ–°ï¼‰
cross_platform_adaptation = {
    "ç»Ÿä¸€æ¥å£": "PlatformDataç»Ÿä¸€æ•°æ®æ ¼å¼",
    "è‡ªåŠ¨å¯¹é½": "æ¨¡å‹è‡ªåŠ¨å¤„ç†ç‰¹å¾å·®å¼‚",
    "ç¼ºå¤±å¤„ç†": "ä¼˜é›…å¤„ç†ç¼ºå¤±æ¨¡æ€",
    "å¤šè¯­è¨€": "XLM-RoBERTaæ”¯æŒä¸­è‹±æ–‡"
}
```

### ç»Ÿä¸€æ•°æ®æ¥å£


### ç»Ÿä¸€é¢„å¤„ç†å™¨è®¾è®¡

### å…³é”®æŠ€æœ¯èåˆç‚¹

#### **1. ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆå€Ÿé‰´Twibotï¼‰**
```python
# å®Œå…¨é‡‡ç”¨Twibotä½œè€…çš„æ ‡å‡†åŒ–æ–¹æ³•
def standardize_features(features_array):
    for i in range(features_array.shape[1]):
        col = features_array[:, i]
        mean_val = np.mean(col)
        std_val = np.std(col)
        if std_val > 0:
            features_array[:, i] = (col - mean_val) / std_val
        else:
            features_array[:, i] = 0
    return features_array
```

#### **2. æ–‡æœ¬èšåˆç­–ç•¥ï¼ˆå€Ÿé‰´+æ”¹è¿›ï¼‰**

#### **3. å›¾ç»“æ„å¤„ç†ï¼ˆå€Ÿé‰´é€»è¾‘ï¼‰**
```python
# å®Œå…¨é‡‡ç”¨Twibotä½œè€…çš„è¾¹å¤„ç†é€»è¾‘
def process_graph_edges(edge_data, user_mapping):
    edges = []
    for source_id, relations in edge_data.items():
        for relation_type, target_id in relations:
            if relation_type == 'post':
                continue  # è·³è¿‡postå…³ç³»ï¼ˆå€Ÿé‰´åŸä½œè€…ï¼‰
            elif relation_type == 'friend':
                edge_type = 0  # å¥½å‹å…³ç³»
            else:  # follow, mention, retweet
                edge_type = 1  # å…¶ä»–å…³ç³»
            
            if source_id in user_mapping and target_id in user_mapping:
                edges.append((user_mapping[source_id], user_mapping[target_id], edge_type))
    
    return edges
```

### æ•°æ®åŠ è½½å™¨è®¾è®¡
```python
class FlexibleDataLoader:
    """çµæ´»æ•°æ®åŠ è½½å™¨ - æ”¯æŒé¢„å¤„ç†åçš„æ•°æ®"""
    
    def load_twibot20(self, device="cpu") -> LoadedPlatformData:
        """åŠ è½½é¢„å¤„ç†åçš„Twibot-20æ•°æ®"""
        return self.load_dataset('twibot20', device)
    
    def load_misbot(self, device="cpu") -> LoadedPlatformData:
        """åŠ è½½é¢„å¤„ç†åçš„Misbotæ•°æ®"""
        return self.load_dataset('misbot', device)
    
    def create_unified_batch(self, datasets, indices_list):
        """åˆ›å»ºç»Ÿä¸€æ‰¹æ¬¡ï¼ˆè‡ªåŠ¨å¤„ç†ç¼ºå¤±æ¨¡æ€ï¼‰"""
        # è‡ªåŠ¨å¡«å……ç¼ºå¤±çš„ç‰¹å¾ç»´åº¦
        # ç¡®ä¿æ‰¹æ¬¡æ•°æ®æ ¼å¼ä¸€è‡´
        pass
```

## ğŸ§  å¤šæ¨¡æ€ç‰¹å¾ç¼–ç å™¨

### æ¶æ„è®¾è®¡
```python
class MultiModalEncoder(nn.Module):
    """å¤šæ¨¡æ€ç‰¹å¾ç¼–ç å™¨"""
    
    def __init__(self, config):
        # æ–‡æœ¬ç¼–ç å™¨ï¼ˆå¿…é¡»ï¼‰
        self.text_encoder = XLMRobertaModel.from_pretrained('xlm-roberta-base')
        self.text_projection = nn.Linear(768, 256)
        
        # æ•°å€¼ç¼–ç å™¨ï¼ˆå¯é€‰ï¼‰
        self.numerical_encoder = nn.Sequential(
            nn.Linear(config.max_numerical_features, 128),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, 256)
        )
        
        # å›¾ç¼–ç å™¨ï¼ˆå¯é€‰ï¼‰
        self.graph_encoder = GATConv(256, 256, heads=4, dropout=0.1)
        
        # åˆ†ç±»ç‰¹å¾ç¼–ç å™¨ï¼ˆå¯é€‰ï¼‰
        self.categorical_encoder = nn.Embedding(
            config.max_categorical_values, 64
        )
        
        # è‡ªé€‚åº”èåˆå±‚
        self.adaptive_fusion = AdaptiveFusion(256)
    
    def forward(self, platform_data: PlatformData):
        features = {}
        
        # æ–‡æœ¬ç‰¹å¾ï¼ˆå¿…é¡»æœ‰ï¼‰
        text_embeddings = self.encode_text(platform_data.user_texts)
        features['text'] = self.text_projection(text_embeddings)
        
        # æ•°å€¼ç‰¹å¾ï¼ˆå¯é€‰ï¼‰
        if platform_data.numerical_features is not None:
            features['numerical'] = self.encode_numerical(
                platform_data.numerical_features
            )
        
        # å›¾ç‰¹å¾ï¼ˆå¯é€‰ï¼‰
        if platform_data.graph_edges is not None:
            features['graph'] = self.encode_graph(
                features['text'], platform_data.graph_edges
            )
        
        # åˆ†ç±»ç‰¹å¾ï¼ˆå¯é€‰ï¼‰
        if platform_data.categorical_features is not None:
            features['categorical'] = self.encode_categorical(
                platform_data.categorical_features
            )
        
        # è‡ªé€‚åº”èåˆ
        return self.adaptive_fusion(features)
```

### æ–‡æœ¬ç¼–ç å™¨è¯¦ç»†è®¾è®¡
```python
class TextEncoder(nn.Module):
    """è·¨è¯­è¨€æ–‡æœ¬ç¼–ç å™¨"""
    
    def __init__(self):
        # ä½¿ç”¨å¤šè¯­è¨€é¢„è®­ç»ƒæ¨¡å‹
        self.backbone = XLMRobertaModel.from_pretrained('xlm-roberta-base')
        self.pooler = nn.Sequential(
            nn.Linear(768, 512),
            nn.Tanh(),
            nn.Linear(512, 256)
        )
    
    def encode_user_text(self, user_texts: List[str]) -> torch.Tensor:
        """ç¼–ç ç”¨æˆ·æ–‡æœ¬ï¼ˆæè¿°+å†…å®¹ï¼‰"""
        # å¤„ç†é•¿æ–‡æœ¬æˆªæ–­
        # è‡ªåŠ¨å¤„ç†ä¸­è‹±æ–‡å·®å¼‚
        # è¿”å›å›ºå®šç»´åº¦çš„ç”¨æˆ·è¡¨ç¤º
        pass
    
    def encode_with_attention(self, texts: List[str]) -> torch.Tensor:
        """ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶èšåˆå¤šæ¡æ–‡æœ¬"""
        # å¯¹äºæœ‰å¤šæ¡æ¨æ–‡/å¾®åšçš„ç”¨æˆ·
        # ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶èšåˆ
        pass
```

### å›¾ç¼–ç å™¨è¯¦ç»†è®¾è®¡
```python
class GraphEncoder(nn.Module):
    """è‡ªé€‚åº”å›¾ç»“æ„ç¼–ç å™¨"""
    
    def __init__(self, hidden_dim=256):
        # æ”¯æŒä¸åŒç±»å‹çš„è¾¹å…³ç³»
        self.edge_type_embedding = nn.Embedding(10, hidden_dim)  # æ”¯æŒå¤šç§è¾¹ç±»å‹
        
        # å›¾æ³¨æ„åŠ›ç½‘ç»œ
        self.gat_layers = nn.ModuleList([
            GATConv(hidden_dim, hidden_dim, heads=4, dropout=0.1)
            for _ in range(2)
        ])
        
        # å›¾çº§åˆ«çš„æ± åŒ–
        self.graph_pooling = GlobalAttentionPooling(
            nn.Linear(hidden_dim, 1)
        )
    
    def forward(self, node_features, edge_index, edge_types=None):
        """å¤„ç†å¼‚æ„å›¾ç»“æ„"""
        # è‡ªåŠ¨é€‚åº”ä¸åŒçš„å›¾æ‹“æ‰‘ç»“æ„
        # å¤„ç†è¾¹ç±»å‹å·®å¼‚
        # è¿”å›èŠ‚ç‚¹çº§åˆ«çš„è¡¨ç¤º
        pass
```

### è‡ªé€‚åº”èåˆå±‚
```python
class AdaptiveFusion(nn.Module):
    """è‡ªé€‚åº”å¤šæ¨¡æ€èåˆ"""
    
    def __init__(self, feature_dim=256):
        self.feature_dim = feature_dim
        
        # æ¨¡æ€æƒé‡å­¦ä¹ 
        self.modality_attention = nn.MultiheadAttention(
            embed_dim=feature_dim, num_heads=8, batch_first=True
        )
        
        # ç¼ºå¤±æ¨¡æ€å¤„ç†
        self.missing_modality_handler = nn.Parameter(
            torch.randn(feature_dim)
        )
        
        # æœ€ç»ˆèåˆ
        self.fusion_layer = nn.Sequential(
            nn.Linear(feature_dim, feature_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(feature_dim * 2, feature_dim)
        )
    
    def forward(self, features: Dict[str, torch.Tensor]) -> torch.Tensor:
        """è‡ªé€‚åº”èåˆå¤šæ¨¡æ€ç‰¹å¾"""
        # å¤„ç†ç¼ºå¤±æ¨¡æ€
        available_modalities = []
        for modality in ['text', 'numerical', 'graph', 'categorical']:
            if modality in features:
                available_modalities.append(features[modality])
            else:
                # ä½¿ç”¨å­¦ä¹ çš„ç¼ºå¤±æ¨¡æ€è¡¨ç¤º
                batch_size = list(features.values())[0].size(0)
                missing_repr = self.missing_modality_handler.unsqueeze(0).repeat(
                    batch_size, 1
                )
                available_modalities.append(missing_repr)
        
        # å †å æ‰€æœ‰æ¨¡æ€
        stacked_features = torch.stack(available_modalities, dim=1)
        
        # æ³¨æ„åŠ›èåˆ
        attended_features, _ = self.modality_attention(
            stacked_features, stacked_features, stacked_features
        )
        
        # å¹³å‡æ± åŒ– + æ®‹å·®è¿æ¥
        pooled_features = attended_features.mean(dim=1)
        fused_features = self.fusion_layer(pooled_features) + pooled_features
        
        return fused_features
```

## ğŸ¯ åŸå‹ç½‘ç»œè®¾è®¡

### æ ¸å¿ƒæ¶æ„
```python
class PrototypicalBotDetector(nn.Module):
    """åŸºäºåŸå‹çš„æœºå™¨äººæ£€æµ‹ç½‘ç»œ"""
    
    def __init__(self, config):
        self.encoder = MultiModalEncoder(config)
        self.prototype_dim = config.prototype_dim
        
        # åŸå‹å­˜å‚¨
        self.register_buffer('human_prototype', torch.zeros(self.prototype_dim))
        self.register_buffer('bot_prototype', torch.zeros(self.prototype_dim))
        self.register_buffer('prototype_initialized', torch.tensor(False))
        
        # è·ç¦»åº¦é‡
        self.distance_metric = config.distance_metric  # 'euclidean' or 'cosine'
        
        # æ¸©åº¦å‚æ•°ï¼ˆå¯å­¦ä¹ ï¼‰
        self.temperature = nn.Parameter(torch.tensor(1.0))
    
    def compute_prototypes(self, support_features, support_labels):
        """è®¡ç®—ç±»åˆ«åŸå‹"""
        unique_labels = torch.unique(support_labels)
        prototypes = {}
        
        for label in unique_labels:
            mask = (support_labels == label)
            class_features = support_features[mask]
            
            if len(class_features) > 0:
                # è®¡ç®—åŸå‹ï¼ˆå¯ä»¥æ˜¯å‡å€¼ã€åŠ æƒå‡å€¼ç­‰ï¼‰
                prototype = self.aggregate_prototype(class_features)
                prototypes[label.item()] = prototype
        
        return prototypes
    
    def aggregate_prototype(self, class_features):
        """èšåˆç±»åˆ«ç‰¹å¾ä¸ºåŸå‹"""
        # ç®€å•å‡å€¼
        if self.config.prototype_aggregation == 'mean':
            return class_features.mean(dim=0)
        
        # åŠ æƒå‡å€¼ï¼ˆåŸºäºç‰¹å¾è´¨é‡ï¼‰
        elif self.config.prototype_aggregation == 'weighted':
            weights = self.compute_feature_weights(class_features)
            return (class_features * weights.unsqueeze(-1)).sum(dim=0)
        
        # æ³¨æ„åŠ›èšåˆ
        elif self.config.prototype_aggregation == 'attention':
            return self.attention_aggregate(class_features)
    
    def compute_distances(self, query_features, prototypes):
        """è®¡ç®—æŸ¥è¯¢æ ·æœ¬åˆ°åŸå‹çš„è·ç¦»"""
        distances = {}
        
        for label, prototype in prototypes.items():
            if self.distance_metric == 'euclidean':
                dist = torch.cdist(
                    query_features, prototype.unsqueeze(0), p=2
                ).squeeze(-1)
            elif self.distance_metric == 'cosine':
                dist = 1 - F.cosine_similarity(
                    query_features, prototype.unsqueeze(0), dim=-1
                )
            
            distances[label] = dist
        
        return distances
    
    def forward(self, support_data, query_data):
        """å‰å‘ä¼ æ’­"""
        # ç¼–ç æ”¯æŒé›†å’ŒæŸ¥è¯¢é›†
        support_features = self.encoder(support_data)
        query_features = self.encoder(query_data)
        
        # è®¡ç®—åŸå‹
        prototypes = self.compute_prototypes(
            support_features, support_data.labels
        )
        
        # è®¡ç®—è·ç¦»
        distances = self.compute_distances(query_features, prototypes)
        
        # è½¬æ¢ä¸ºlogits
        human_dist = distances.get(0, torch.inf)  # çœŸäººè·ç¦»
        bot_dist = distances.get(1, torch.inf)    # æœºå™¨äººè·ç¦»
        
        # è·ç¦»è¶Šå°ï¼Œæ¦‚ç‡è¶Šå¤§ï¼ˆåŠ æ¸©åº¦å‚æ•°ï¼‰
        logits = torch.stack([
            -human_dist / self.temperature,
            -bot_dist / self.temperature
        ], dim=-1)
        
        return F.log_softmax(logits, dim=-1)
```

### åŸå‹æ›´æ–°ç­–ç•¥
```python
class PrototypeUpdater:
    """åŸå‹æ›´æ–°ç­–ç•¥"""
    
    def __init__(self, update_strategy='momentum'):
        self.update_strategy = update_strategy
        self.momentum = 0.9
    
    def update_prototypes(self, old_prototypes, new_samples, labels):
        """æ›´æ–°åŸå‹"""
        if self.update_strategy == 'replace':
            # ç›´æ¥æ›¿æ¢
            return self.compute_new_prototypes(new_samples, labels)
        
        elif self.update_strategy == 'momentum':
            # åŠ¨é‡æ›´æ–°
            new_prototypes = self.compute_new_prototypes(new_samples, labels)
            updated_prototypes = {}
            
            for label in new_prototypes:
                if label in old_prototypes:
                    updated_prototypes[label] = (
                        self.momentum * old_prototypes[label] +
                        (1 - self.momentum) * new_prototypes[label]
                    )
                else:
                    updated_prototypes[label] = new_prototypes[label]
            
            return updated_prototypes
        
        elif self.update_strategy == 'adaptive':
            # è‡ªé€‚åº”æ›´æ–°ï¼ˆåŸºäºæ ·æœ¬è´¨é‡ï¼‰
            return self.adaptive_update(old_prototypes, new_samples, labels)
```

## ğŸ“š Few-Shotä»»åŠ¡æ„å»º

### Episodeé‡‡æ ·ç­–ç•¥
```python
class EpisodeSampler:
    """Few-shotä»»åŠ¡é‡‡æ ·å™¨"""
    
    def __init__(self, n_way=2, k_shot=5, q_query=15):
        self.n_way = n_way      # ç±»åˆ«æ•°ï¼ˆhuman vs botï¼‰
        self.k_shot = k_shot    # æ¯ç±»æ”¯æŒæ ·æœ¬æ•°
        self.q_query = q_query  # æ¯ç±»æŸ¥è¯¢æ ·æœ¬æ•°
    
    def sample_episode(self, platform_data: PlatformData):
        """ä»å¹³å°æ•°æ®ä¸­é‡‡æ ·ä¸€ä¸ªepisode"""
        # åˆ†ç¦»ä¸åŒç±»åˆ«çš„æ ·æœ¬
        human_users = [uid for uid, label in platform_data.labels.items() if label == 0]
        bot_users = [uid for uid, label in platform_data.labels.items() if label == 1]
        
        # é‡‡æ ·æ”¯æŒé›†
        support_humans = random.sample(human_users, self.k_shot)
        support_bots = random.sample(bot_users, self.k_shot)
        
        # é‡‡æ ·æŸ¥è¯¢é›†
        remaining_humans = [u for u in human_users if u not in support_humans]
        remaining_bots = [u for u in bot_users if u not in support_bots]
        
        query_humans = random.sample(remaining_humans, self.q_query)
        query_bots = random.sample(remaining_bots, self.q_query)
        
        # æ„å»ºepisode
        support_set = self.build_dataset(
            platform_data, support_humans + support_bots
        )
        query_set = self.build_dataset(
            platform_data, query_humans + query_bots
        )
        
        return support_set, query_set
    
    def sample_cross_domain_episode(self, source_data, target_data):
        """è·¨åŸŸepisodeé‡‡æ ·"""
        # ä»æºåŸŸé‡‡æ ·å¤§é‡æ•°æ®ä½œä¸ºé¢„è®­ç»ƒ
        # ä»ç›®æ ‡åŸŸé‡‡æ ·å°‘é‡æ•°æ®ä½œä¸ºé€‚åº”
        pass
```

### è®­ç»ƒç­–ç•¥
```python
class MetaTrainer:
    """å…ƒå­¦ä¹ è®­ç»ƒå™¨"""
    
    def __init__(self, model, optimizer, config):
        self.model = model
        self.optimizer = optimizer
        self.config = config
        
        # è®­ç»ƒç­–ç•¥
        self.training_strategy = config.training_strategy
        self.episodes_per_epoch = config.episodes_per_epoch
        
    def meta_train_epoch(self, source_data):
        """å…ƒè®­ç»ƒä¸€ä¸ªepoch"""
        total_loss = 0
        
        for episode_idx in range(self.episodes_per_epoch):
            # é‡‡æ ·episode
            support_set, query_set = self.sampler.sample_episode(source_data)
            
            # å‰å‘ä¼ æ’­
            logits = self.model(support_set, query_set)
            loss = F.nll_loss(logits, query_set.labels)
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
            
            total_loss += loss.item()
        
        return total_loss / self.episodes_per_epoch
    
    def meta_test(self, target_data, n_episodes=100):
        """åœ¨ç›®æ ‡åŸŸä¸Šæµ‹è¯•"""
        accuracies = []
        
        for _ in range(n_episodes):
            support_set, query_set = self.sampler.sample_episode(target_data)
            
            with torch.no_grad():
                logits = self.model(support_set, query_set)
                predictions = logits.argmax(dim=-1)
                accuracy = (predictions == query_set.labels).float().mean()
                accuracies.append(accuracy.item())
        
        return np.mean(accuracies), np.std(accuracies)
```

## ğŸ”„ è®­ç»ƒä¸è¯„ä¼°æµç¨‹

### è®­ç»ƒæµç¨‹
```python
def training_pipeline():
    """å®Œæ•´è®­ç»ƒæµç¨‹"""
    
    # 1. æ•°æ®åŠ è½½
    source_data = FlexibleDataLoader().load_twibot20()
    target_data = FlexibleDataLoader().load_misbot()
    
    # 2. æ¨¡å‹åˆå§‹åŒ–
    model = PrototypicalBotDetector(config)
    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)
    trainer = MetaTrainer(model, optimizer, config)
    
    # 3. å…ƒè®­ç»ƒé˜¶æ®µ
    print("å¼€å§‹å…ƒè®­ç»ƒ...")
    for epoch in range(config.meta_train_epochs):
        loss = trainer.meta_train_epoch(source_data)
        print(f"Epoch {epoch}: Loss = {loss:.4f}")
        
        # éªŒè¯
        if epoch % config.eval_interval == 0:
            acc, std = trainer.meta_test(target_data)
            print(f"Target Domain Accuracy: {acc:.4f} Â± {std:.4f}")
    
    # 4. ä¿å­˜æ¨¡å‹
    torch.save(model.state_dict(), 'prototypical_bot_detector.pth')
    
    return model
```

### è¯„ä¼°æŒ‡æ ‡
```python
class Evaluator:
    """è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc']
    
    def evaluate_cross_domain(self, model, source_data, target_data):
        """è·¨åŸŸè¯„ä¼°"""
        results = {}
        
        # ä¸åŒshotæ•°çš„è¯„ä¼°
        for k_shot in [1, 5, 10, 20]:
            sampler = EpisodeSampler(k_shot=k_shot)
            accuracies = []
            
            for _ in range(100):  # 100ä¸ªepisode
                support_set, query_set = sampler.sample_episode(target_data)
                acc = self.evaluate_episode(model, support_set, query_set)
                accuracies.append(acc)
            
            results[f'{k_shot}-shot'] = {
                'mean': np.mean(accuracies),
                'std': np.std(accuracies),
                'ci_95': np.percentile(accuracies, [2.5, 97.5])
            }
        
        return results
    
    def evaluate_adaptation_speed(self, model, target_data):
        """è¯„ä¼°é€‚åº”é€Ÿåº¦"""
        # æµ‹è¯•æ¨¡å‹åœ¨ä¸åŒæ•°é‡çš„ç›®æ ‡åŸŸæ ·æœ¬ä¸‹çš„æ€§èƒ½
        adaptation_curve = {}
        
        for n_samples in [1, 2, 5, 10, 20, 50]:
            # ä½¿ç”¨n_samplesä¸ªæ ·æœ¬è¿›è¡Œé€‚åº”
            acc = self.test_with_n_samples(model, target_data, n_samples)
            adaptation_curve[n_samples] = acc
        
        return adaptation_curve
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
CPPNBOT/
â”œâ”€â”€ README.md                           # é¡¹ç›®è¯´æ˜
â”œâ”€â”€ DESIGN_DOCUMENT.md                  # æœ¬è®¾è®¡æ–‡æ¡£
â”œâ”€â”€ requirements.txt                    # ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ default.yaml                    # é»˜è®¤é…ç½®
â”‚   â”œâ”€â”€ twibot20.yaml                   # Twibot-20é…ç½®
â”‚   â””â”€â”€ misbot.yaml                     # Misboté…ç½®
â”œâ”€â”€ dataset/                            # æ•°æ®é›†ï¼ˆå·²æœ‰ï¼‰
â”‚   â”œâ”€â”€ Twibot-20/
â”‚   â”œâ”€â”€ Misbot/
â”‚   â””â”€â”€ Misbot_Graph/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ platform_data.py            # æ•°æ®æ¥å£å®šä¹‰
â”‚   â”‚   â”œâ”€â”€ flexible_loader.py          # çµæ´»æ•°æ®åŠ è½½å™¨
â”‚   â”‚   â””â”€â”€ episode_sampler.py          # Episodeé‡‡æ ·å™¨
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ encoders/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ text_encoder.py         # æ–‡æœ¬ç¼–ç å™¨
â”‚   â”‚   â”‚   â”œâ”€â”€ graph_encoder.py        # å›¾ç¼–ç å™¨
â”‚   â”‚   â”‚   â”œâ”€â”€ numerical_encoder.py    # æ•°å€¼ç¼–ç å™¨
â”‚   â”‚   â”‚   â””â”€â”€ fusion.py               # å¤šæ¨¡æ€èåˆ
â”‚   â”‚   â”œâ”€â”€ prototypical.py             # åŸå‹ç½‘ç»œ
â”‚   â”‚   â””â”€â”€ meta_learner.py             # å…ƒå­¦ä¹ å™¨
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ meta_trainer.py             # å…ƒè®­ç»ƒå™¨
â”‚   â”‚   â”œâ”€â”€ evaluator.py                # è¯„ä¼°å™¨
â”‚   â”‚   â””â”€â”€ utils.py                    # è®­ç»ƒå·¥å…·
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py                   # é…ç½®ç®¡ç†
â”‚       â”œâ”€â”€ metrics.py                  # è¯„ä¼°æŒ‡æ ‡
â”‚       â””â”€â”€ visualization.py            # å¯è§†åŒ–å·¥å…·
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ train_meta_model.py             # å…ƒè®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ evaluate_cross_domain.py        # è·¨åŸŸè¯„ä¼°è„šæœ¬
â”‚   â”œâ”€â”€ ablation_study.py               # æ¶ˆèå®éªŒ
â”‚   â””â”€â”€ baseline_comparison.py          # åŸºçº¿å¯¹æ¯”
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ data_exploration.ipynb          # æ•°æ®æ¢ç´¢
â”‚   â”œâ”€â”€ model_analysis.ipynb            # æ¨¡å‹åˆ†æ
â”‚   â””â”€â”€ results_visualization.ipynb     # ç»“æœå¯è§†åŒ–
â””â”€â”€ results/
    â”œâ”€â”€ checkpoints/                    # æ¨¡å‹æ£€æŸ¥ç‚¹
    â”œâ”€â”€ logs/                           # è®­ç»ƒæ—¥å¿—
    â””â”€â”€ figures/                        # ç»“æœå›¾è¡¨
```

## ğŸ¯ å®æ–½è®¡åˆ’

### ç¬¬1-2å‘¨ï¼šæ•°æ®é¢„å¤„ç†ï¼ˆèåˆç­–ç•¥å®æ–½ï¼‰
- [ ] ç»Ÿä¸€é¢„å¤„ç†å™¨å®ç°
- [ ] é…ç½®æ–‡ä»¶è®¾è®¡
- [ ] é¢„å¤„ç†è„šæœ¬
- [ ] çµæ´»æ•°æ®åŠ è½½å™¨
- [ ] è¿è¡Œé¢„å¤„ç†ï¼Œç”Ÿæˆç»Ÿä¸€æ ¼å¼æ•°æ®
- [ ] éªŒè¯æ•°æ®å®Œæ•´æ€§å’Œç‰¹å¾å¯¹é½

### ç¬¬3-4å‘¨ï¼šå¤šæ¨¡æ€ç¼–ç å™¨
- [ ] æ–‡æœ¬ç¼–ç å™¨å®ç°ï¼ˆXLM-RoBERTaï¼‰
- [ ] å›¾ç¼–ç å™¨å®ç°ï¼ˆGATï¼‰
- [ ] æ•°å€¼/åˆ†ç±»ç‰¹å¾ç¼–ç å™¨
- [ ] è‡ªé€‚åº”èåˆå±‚å®ç°
- [ ] å•å…ƒæµ‹è¯•ä¸éªŒè¯

### ç¬¬5-6å‘¨ï¼šåŸå‹ç½‘ç»œä¸å…ƒå­¦ä¹ 
- [ ] åŸå‹ç½‘ç»œæ ¸å¿ƒå®ç°
- [ ] Episodeé‡‡æ ·å™¨å®ç°
- [ ] å…ƒå­¦ä¹ è®­ç»ƒå¾ªç¯
- [ ] è·¨åŸŸé€‚åº”æœºåˆ¶
- [ ] ç«¯åˆ°ç«¯æµ‹è¯•

### ç¬¬7-8å‘¨ï¼šå®éªŒä¸è¯„ä¼°
- [ ] åŸºçº¿æ–¹æ³•å®ç°
- [ ] è·¨åŸŸè¿ç§»å®éªŒ
- [ ] æ¶ˆèå®éªŒè®¾è®¡ä¸æ‰§è¡Œ
- [ ] å‚æ•°æ•æ„Ÿæ€§åˆ†æ
- [ ] ç»“æœå¯è§†åŒ–

### ç¬¬9-10å‘¨ï¼šä¼˜åŒ–ä¸å®Œå–„
- [ ] æ¨¡å‹ä¼˜åŒ–ä¸è°ƒå‚
- [ ] ä»£ç é‡æ„ä¸æ–‡æ¡£å®Œå–„
- [ ] å®éªŒç»“æœåˆ†æ
- [ ] è®ºæ–‡æ’°å†™å‡†å¤‡

## ğŸ“Š é¢„æœŸå®éªŒç»“æœ

### æ€§èƒ½æŒ‡æ ‡
- **è·¨åŸŸå‡†ç¡®ç‡**ï¼š75-80%ï¼ˆTwibot-20 â†’ Misbotï¼‰
- **Few-shotæ€§èƒ½**ï¼š5-shotè¾¾åˆ°70%+å‡†ç¡®ç‡
- **é€‚åº”é€Ÿåº¦**ï¼š10ä¸ªæ ·æœ¬å†…å¿«é€Ÿæ”¶æ•›
- **ç›¸å¯¹æå‡**ï¼šæ¯”ç›´æ¥è¿ç§»æå‡8-12%

### æ¶ˆèå®éªŒ
- [ ] å¤šæ¨¡æ€ vs å•æ¨¡æ€çš„è´¡çŒ®
- [ ] Twibotç‰¹å¾å·¥ç¨‹ vs åŸå§‹ç‰¹å¾çš„æ•ˆæœ
- [ ] ä¸åŒåŸå‹èšåˆç­–ç•¥çš„æ•ˆæœ
- [ ] å›¾ç»“æ„ä¿¡æ¯çš„é‡è¦æ€§
- [ ] è·¨è¯­è¨€ç¼–ç å™¨çš„ä½œç”¨

### åŸºçº¿å¯¹æ¯”
- [ ] Direct Transfer
- [ ] Fine-tuning
- [ ] Domain Adaptation (MMD, DANN)
- [ ] Vanilla Prototypical Networks
- [ ] MAML (ä½œä¸ºå¯¹æ¯”)
- [ ] TwibotåŸå§‹æ–¹æ³•ï¼ˆåœ¨Misbotä¸Šçš„è¡¨ç°ï¼‰

## ğŸ” å…³é”®æŠ€æœ¯æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆ

### æŒ‘æˆ˜1ï¼šè·¨è¯­è¨€æ–‡æœ¬ç†è§£
**è§£å†³æ–¹æ¡ˆ**ï¼šä½¿ç”¨XLM-RoBERTaå¤šè¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ï¼Œåœ¨å…ƒè®­ç»ƒé˜¶æ®µå­¦ä¹ è¯­è¨€æ— å…³çš„æœºå™¨äººè¡Œä¸ºè¡¨ç¤ºã€‚

### æŒ‘æˆ˜2ï¼šå›¾ç»“æ„å·®å¼‚
**è§£å†³æ–¹æ¡ˆ**ï¼šè®¾è®¡è‡ªé€‚åº”å›¾ç¼–ç å™¨ï¼Œèƒ½å¤Ÿå¤„ç†ä¸åŒç±»å‹å’Œå¯†åº¦çš„å›¾ç»“æ„ï¼Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶è‡ªåŠ¨å­¦ä¹ é‡è¦çš„ç»“æ„æ¨¡å¼ã€‚

### æŒ‘æˆ˜3ï¼šæ•°æ®åˆ†å¸ƒåç§»
**è§£å†³æ–¹æ¡ˆ**ï¼šé€šè¿‡åŸå‹å­¦ä¹ æŠ½è±¡å‡ºå¹³å°æ— å…³çš„æœºå™¨äººè¡Œä¸ºæ¨¡å¼ï¼Œä½¿ç”¨è‡ªé€‚åº”èåˆå±‚å¤„ç†ç‰¹å¾åˆ†å¸ƒå·®å¼‚ã€‚

### æŒ‘æˆ˜4ï¼šå°‘æ ·æœ¬è¿‡æ‹Ÿåˆ
**è§£å†³æ–¹æ¡ˆ**ï¼šåœ¨å…ƒè®­ç»ƒé˜¶æ®µæ¨¡æ‹Ÿå°‘æ ·æœ¬åœºæ™¯ï¼Œå­¦ä¹ å¦‚ä½•ä»å°‘é‡æ ·æœ¬ä¸­æå–æœ‰æ•ˆä¿¡æ¯ï¼Œä½¿ç”¨æ­£åˆ™åŒ–æŠ€æœ¯é˜²æ­¢è¿‡æ‹Ÿåˆã€‚

## ğŸ“ è®ºæ–‡è´¡çŒ®ç‚¹

### æ–¹æ³•è´¡çŒ®
1. **èåˆå¼æ•°æ®å¤„ç†**ï¼šæˆåŠŸèåˆTwibotä½œè€…çš„æˆåŠŸç»éªŒä¸è·¨å¹³å°è®¾è®¡ç†å¿µ
2. **å¤šæ¨¡æ€åŸå‹å­¦ä¹ æ¡†æ¶**ï¼šé¦–æ¬¡å°†å¤šæ¨¡æ€å­¦ä¹ ä¸åŸå‹ç½‘ç»œç»“åˆç”¨äºè·¨å¹³å°æœºå™¨äººæ£€æµ‹
3. **è‡ªé€‚åº”ç‰¹å¾èåˆ**ï¼šè®¾è®¡èƒ½å¤Ÿå¤„ç†ç¼ºå¤±æ¨¡æ€çš„è‡ªé€‚åº”èåˆæœºåˆ¶
4. **è·¨åŸŸå¿«é€Ÿé€‚åº”**ï¼šå®ç°çœŸæ­£çš„å°‘æ ·æœ¬å¿«é€Ÿéƒ¨ç½²èƒ½åŠ›

### å®éªŒè´¡çŒ®
1. **è·¨å¹³å°åŸºå‡†**ï¼šå»ºç«‹Twibot-20åˆ°Misbotçš„è·¨å¹³å°è¯„ä¼°åŸºå‡†
2. **ç‰¹å¾å·¥ç¨‹éªŒè¯**ï¼šéªŒè¯Twibotç‰¹å¾å·¥ç¨‹åœ¨è·¨åŸŸåœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§
3. **è¯¦ç»†æ¶ˆèåˆ†æ**ï¼šå…¨é¢åˆ†æå„ç»„ä»¶å¯¹è·¨åŸŸæ€§èƒ½çš„è´¡çŒ®
4. **å®ç”¨æ€§éªŒè¯**ï¼šåœ¨çœŸå®åœºæ™¯ä¸‹éªŒè¯å¿«é€Ÿéƒ¨ç½²èƒ½åŠ›

### åº”ç”¨ä»·å€¼
1. **é™ä½éƒ¨ç½²æˆæœ¬**ï¼šå‡å°‘æ–°å¹³å°éƒ¨ç½²æ‰€éœ€çš„æ ‡æ³¨å·¥ä½œ
2. **æé«˜æ£€æµ‹æ•ˆæœ**ï¼šé€šè¿‡èåˆæˆåŠŸç»éªŒå’Œå¤šæ¨¡æ€å­¦ä¹ æå‡æ£€æµ‹å‡†ç¡®ç‡
3. **å¢å¼ºæ³›åŒ–èƒ½åŠ›**ï¼šæä¾›è·¨å¹³å°ã€è·¨è¯­è¨€çš„é€šç”¨è§£å†³æ–¹æ¡ˆ
4. **å·¥ç¨‹å®ç”¨æ€§**ï¼šæä¾›å®Œæ•´çš„æ•°æ®å¤„ç†å’Œæ¨¡å‹è®­ç»ƒç®¡é“

---

*æ–‡æ¡£ç‰ˆæœ¬ï¼šv1.0*  
*æœ€åæ›´æ–°ï¼š2025å¹´12æœˆ*  
*ä½œè€…ï¼šæ¯•ä¸šè®¾è®¡é¡¹ç›®ç»„*